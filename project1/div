import numpy as np
from imageio.v2 import imread
from sklearn.model_selection import  train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.metrics import mean_squared_error
from matplotlib import cm
from matplotlib.ticker import LinearLocator
from sklearn.utils import resample
from sklearn import linear_model

def create_X(x, y, n ):
	if len(x.shape) > 1:
		x = np.ravel(x)
		y = np.ravel(y)

	N = len(x)
	l = int((n+1)*(n+2)/2)		# Number of elements in beta
	X = np.ones((N,l))

	for i in range(1,n+1):
		q = int((i)*(i+1)/2)
		for k in range(i+1):
			X[:,q+k] = (x**(i-k))*(y**k)

	return X

#path to the TIFF file
file_path = 'C:/Users/krist/Downloads/sognsvann.tif'
# Load the terrain
terrain = imread(file_path)

N = 250
m = 10 # polynomial order
terrain = terrain[:N,:N]

#create mesh of image pixels
x = np.linspace(0,1,np.shape(terrain)[0])
y = np.linspace(0, 1, np.shape(terrain)[1])
x_mesh, y_mesh = np.meshgrid(x,y)
z = terrain.flatten()

X = create_X(x_mesh,y_mesh, 2)
print(np.shape(z), np.shape(x) , np.shape(X))



index = list(range(len(z)))
index = np.array(index)
index_train, index_test = train_test_split(index, test_size=0.3)
#index_train = np.array([index_train])
#index_test = np.array([index_test])
print(np.shape(index), np.shape(index_train), np.shape(index_test))

polynomial = 10



"""#bootstrap OLS

bootstraps = 10

error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
polydegree = np.zeros(polynomial)


for p in range(1, polynomial + 1):

	y_pred = np.empty((len(index_test), bootstraps))
	X = create_X(x_mesh, y_mesh, p)

	for i in range(bootstraps):

		#print(f"b {i}, p {p}")
		resample_indcies = resample(index_train, n_samples = int(len(index_train)*0.6) )

		#print(f"resample indecies: {np.shape((resample_indcies))}")
		#print(f"shape of design matrix : {np.shape(X)}")
		#instead just split the design matrix

		#x_, y_ = x[resample_indcies], y[resample_indcies]
		#x_mesh, y_mesh = np.meshgrid(x_, y_)
		#X = create_X(x_mesh,y_mesh, p)
		Xtrain = X[resample_indcies][:]

		#x_mesh_, y_mesh_ = np.meshgrid(x[index_test], y[index_test])
		#Xtest = create_X(x_mesh_, y_mesh_, p)  ##only depends on i
		Xtest = X[index_test][:]


		clf = skl.LinearRegression().fit(Xtrain, z[resample_indcies])
		z_pred = clf.predict(Xtest)
		#print(f"predicted z: {z_pred}")

		y_pred[:, i] = z_pred


	#error[p] = np.mean(np.mean((z[index_test] - y_pred) ** 2, axis=1, keepdims=True))
	mse = np.zeros(bootstraps)
	for i in range(bootstraps):
		mse[i] = mean_squared_error(z[index_test], y_pred[:, i] )


	error[p-1] = np.mean(mse)
	bias[p-1] = np.mean((z[index_test] - np.mean(y_pred, axis=1, keepdims=True)) ** 2)

	variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
	polydegree[p-1] = p

plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
#plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
plt.title(f"{bootstraps} bootstraps")
plt.legend()
plt.show()

"""
"""
##cross validation OLS

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((polynomial, k))


error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
polydegree = np.zeros(polynomial)

for p in range(1, polynomial + 1):
	j = 0

	y_pred =  np.empty((int(len(z)/k), k))

	X = create_X(x_mesh, y_mesh, p)
	##scaling



	for train_inds, test_inds in kfold.split(z):

		Xtrain = X[train_inds][:]
		Xtest = X[test_inds][:]

		ztrain = z[train_inds]


		clf = skl.LinearRegression().fit(Xtrain, ztrain)


		ztest = z[test_inds]
		z_pred = clf.predict(Xtest)

		y_pred[:, j] = z_pred

		j += 1

	#print(y_pred)
	mse = np.zeros(k)
	for i in range(k):
		mse[i] = mean_squared_error(ztest, y_pred[:, i] )


	error[p-1] = np.mean(mse)
	bias[p-1] = np.mean((z[index_test] - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
	variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
	polydegree[p-1] = p


plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
plt.title(f"{k}-fold cross validation OLS on real data")
plt.legend()
plt.show()


"""
## cross validation bias viariance ridge and lasso


from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((polynomial, k))
lmb = 1

error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
polydegree = np.zeros(polynomial)

for p in range(1, polynomial + 1):
	j = 0

	y_pred =  np.empty((int(len(z)/k), k))

	X = create_X(x_mesh, y_mesh, p)
	##scaling



	for train_inds, test_inds in kfold.split(z):

		Xtrain = X[train_inds][:]
		Xtest = X[test_inds][:]

		ztrain = z[train_inds]


		#ridge
		#RegRidge = linear_model.Ridge(lmb, fit_intercept=False)
		#RegRidge.fit(Xtrain, ztrain)

		ztest = z[test_inds]

		#ytildeRidge = RegRidge.predict(Xtrain)
		#ypredictRidge = RegRidge.predict(Xtest)

		#z_pred = RegRidge.predict(Xtest)


		#lasso, comment out
		RegLasso = skl.Lasso(alpha=lmb, max_iter=10000)
		#RegLasso = linear_model.Lasso(lmb)
		RegLasso.fit(Xtrain, ztrain)
		ypredictLasso = RegLasso.predict(Xtest)


		#y_pred[:, j] = z_pred
		y_pred[:, j] = ypredictLasso

		j += 1

	#print(y_pred)
	mse = np.zeros(k)
	for i in range(k):
		mse[i] = mean_squared_error(ztest, y_pred[:, i] )


	error[p-1] = np.mean(mse)
	bias[p-1] = np.mean((z[index_test] - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
	variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
	polydegree[p-1] = p


plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
#plt.title(f" Ridge {k}-fold cross validation on real data, lmb = {lmb}")
plt.title(f" lasso {k}-fold cross validation on real data lmb = {lmb}")
plt.legend()
plt.show()


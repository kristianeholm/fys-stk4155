import numpy as np
from sklearn.model_selection import  train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.metrics import mean_squared_error
from matplotlib import cm
from matplotlib.ticker import LinearLocator
from sklearn.utils import resample
from imageio.v2 import imread

def create_X(x, y, n ):
    if len(x.shape) > 1:
        x = np.ravel(x)
        y = np.ravel(y)

    N = len(x)
    l = int((n+1)*(n+2)/2)		# Number of elements in beta
    X = np.ones((N,l))

    for i in range(1,n+1):
        q = int((i)*(i+1)/2)
        for k in range(i+1):
            X[:,q+k] = (x**(i-k))*(y**k)

    return X



##cross validation

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

#Load the terrain
file_path = 'C:/Users/krist/Downloads/sognsvann.tif'
terrain = imread(file_path)
N = 1000
m = 10  # polynomial order
terrain = terrain[:N, :N]

x = np.linspace(0, 1, np.shape(terrain)[0])
y = np.linspace(0, 1, np.shape(terrain)[1])
x_mesh, y_mesh = np.meshgrid(x, y)

z = terrain.flatten()

polynomial = 10
k = 5
kfold = KFold(n_splits=k)

error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
r2_scores = np.zeros(polynomial)
polydegree = np.zeros(polynomial)


# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((polynomial, k))

for p in range(1, polynomial + 1):
    j = 0
    #y_pred = np.empty((int(datapoints/k), k))
    y_pred = np.empty((int(len(x) / k), k))

    for train_inds, test_inds in kfold.split(x):

        xtrain = x[train_inds]
        ytrain = y[train_inds]

        #create design matrix
        X = create_X(xtrain, ytrain, p)

        ztrain = z[train_inds]

        clf = skl.LinearRegression().fit(X, ztrain)

        xtest = x[test_inds]
        ytest = y[test_inds]
        #create test design matrix
        Xtest= create_X(xtest, ytest, p)

        ztest = z[test_inds]

        z_pred = clf.predict(Xtest)

        y_pred[:, j] = z_pred

        j += 1

    #print(y_pred)
    mse = np.zeros(k)
    for i in range(k):
        mse[i] = mean_squared_error(ztest, y_pred[:, i] )


    error[p-1] = np.mean(mse)
    #bias[p-1] = np.mean((z[index_test] - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
    bias[p-1] = np.mean((ztest - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
    variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
    polydegree[p-1] = p



plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
plt.title(f"{k}-fold cross validation")
plt.legend()
plt.show()

import numpy as np
from sklearn.model_selection import  train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.metrics import mean_squared_error
from matplotlib import cm
from matplotlib.ticker import LinearLocator
from sklearn.utils import resample
from sklearn import linear_model
from imageio.v2 import imread


def create_X(x, y, n ):
	if len(x.shape) > 1:
		x = np.ravel(x)
		y = np.ravel(y)

	N = len(x)
	l = int((n+1)*(n+2)/2)		# Number of elements in beta
	X = np.ones((N,l))

	for i in range(1,n+1):
		q = int((i)*(i+1)/2)
		for k in range(i+1):
			X[:,q+k] = (x**(i-k))*(y**k)

	return X


##cross validation

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

file_path = 'C:/Users/krist/Downloads/sognsvann.tif'
# Load the terrain
terrain = imread(file_path)

N = 1000
m = 10 # polynomial order
terrain = terrain[:N,:N]

#create mesh of image pixels
x = np.linspace(0,1,np.shape(terrain)[0])
y = np.linspace(0, 1, np.shape(terrain)[1])
x_mesh, y_mesh = np.meshgrid(x,y)

z = terrain.flatten()
X = create_X(x_mesh,y_mesh, m)

polynomial=10

"""
k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((polynomial, k))


error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
polydegree = np.zeros(polynomial)


#OLS

for p in range(1, polynomial + 1):
	j = 0
	y_pred = np.empty((int(len(x)/k), k))

	for train_inds, test_inds in kfold.split(x):

		xtrain = x[train_inds]
		ytrain = y[train_inds]

		#create design matrix
		X = create_X(xtrain, ytrain, p)

		ztrain = z[train_inds]

		clf = skl.LinearRegression().fit(X, ztrain)

		xtest = x[test_inds]
		ytest = y[test_inds]
		#create test design matrix
		Xtest= create_X(xtest, ytest, p)

		ztest = z[test_inds]

		z_pred = clf.predict(Xtest)

		y_pred[:, j] = z_pred

		j += 1

	#print(y_pred)
	mse = np.zeros(k)
	for i in range(k):
		mse[i] = mean_squared_error(ztest, y_pred[:, i] )


	error[p-1] = np.mean(mse)
	bias[p-1] = np.mean((ztest - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
	variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
	polydegree[p-1] = p
	
plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
plt.title(f"{k}-fold cross validation of OLS on real terrain data")
plt.legend()
#plt.show()

# Perform OLS on the entire dataset
X_ols = create_X(x_mesh, y_mesh, polynomial)
clf_ols = skl.LinearRegression().fit(X_ols, z)
z_ols = clf_ols.predict(X_ols)

# Calculate MSE and R-squared for the OLS result
mse_ols = mean_squared_error(z, z_ols)
r2_ols = clf_ols.score(X_ols, z)

print(f"Mean Squared Error (MSE) for OLS: {mse_ols:.4f}")
print(f"R-squared (R2) for OLS: {r2_ols:.4f}")

# 2D plot of the terrain after OLS and cross-validation
plt.figure(figsize=(8, 6))
plt.imshow(z_ols.reshape(N, N), cmap='viridis', extent=(0, 1, 0, 1))
plt.title("Terrain 2D Plot (OLS and Cross-Validation Result)")
plt.xlabel('East')
plt.ylabel('North')
plt.colorbar()
plt.show()

# 3D plot of the terrain after OLS and cross-validation
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
Xgrid, Ygrid = np.meshgrid(x, y)
ZGrid = z_ols.reshape(N, N)

surf = ax.plot_surface(Xgrid, Ygrid, ZGrid, cmap=cm.coolwarm, linewidth=0, antialiased=False)
ax.set_xlabel('East')
ax.set_ylabel('North')
ax.set_zlabel('Elevation')
ax.set_title("Terrain 3D Plot (OLS and Cross-Validation Result)")
plt.colorbar(surf, ax=ax, shrink=0.5, aspect=9)
plt.show()




"""

##bias viariance cross variance ridge and lasso


k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((polynomial, k))



error = np.zeros(polynomial)
bias = np.zeros(polynomial)
variance = np.zeros(polynomial)
polydegree = np.zeros(polynomial)
lamda = 0.00001

for p in range(1, polynomial + 1):
	j = 0
	y_pred = np.empty((int(len(x)/k), k))

	for train_inds, test_inds in kfold.split(x):

		xtrain = x[train_inds]
		ytrain = y[train_inds]

		#create design matrix
		X_train = create_X(xtrain, ytrain, p)
		ztrain = z[train_inds]

		clf = skl.LinearRegression().fit(X, ztrain)


		##ridge and lasso only differ with few lines, for ridge calculation comment out the lasso,
		##for lasso, comment out the ridge


		#should only have 1 of the belows

		# ridge
		beta_ridge = (np.linalg.pinv(
			(X_train.T @ X_train + (lamda * np.identity(np.shape(X_train)[1])))) @ X_train.T) @ ztrain

		ztildeTrain = X_train @ beta_ridge


		##lasso,
		#RegLasso = linear_model.Lasso(lamda)
		#clf = RegLasso.fit(X, ztrain)



		#this part is same for both
		xtest = x[test_inds]
		ytest = y[test_inds]
		#create test design matrix
		Xtest= create_X(xtest, ytest, p)
		ztest = z[test_inds]


		##only have one of these, the other should be commented out
		z_pred = Xtest @ beta_ridge #ridge
		#z_pred = clf.predict(Xtest) #lasso

		y_pred[:, j] = z_pred

		j += 1

	#print(y_pred)
	mse = np.zeros(k)
	for i in range(k):
		mse[i] = mean_squared_error(ztest, y_pred[:, i] )


	error[p-1] = np.mean(mse)
	bias[p-1] = np.mean((ztest - np.mean(y_pred, axis=1, keepdims=True)) ** 2)
	variance[p-1] = np.mean(np.var(y_pred, axis=1, keepdims=True))
	polydegree[p-1] = p



plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.plot(polydegree, variance + bias, label='Bias + Variance')
plt.xlabel("polynomial degree")
plt.ylabel("")
plt.title(f"{k}-fold cross validation, Ridge l = {lamda}")
#plt.title(f"{k}-fold cross validation, Lasso l = {lamda}")
plt.legend()
plt.show()

#"""

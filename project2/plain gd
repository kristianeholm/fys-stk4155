#Plain gradient descent 
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import sys

# the number of datapoints
n = 100
x = 2*np.random.rand(n,1)
y = 4 + 3*x + 2*x**2 + np.random.randn(n,1) #f(x)=a_0+a_1x+a_2x^2

X = np.c_[np.ones((n,1)), x, x**2] #need a new column with the X values squared

# Hessian matrix
H = (2.0/n)* X.T @ X
# Get the eigenvalues
EigValues, EigVectors = np.linalg.eig(H)
#print(f"Eigenvalues of Hessian Matrix:{EigValues}")

beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y
#print(beta_linreg)

beta_without_momentum = np.random.randn(3,1)
beta_with_momentum = np.random.randn(3,1)

#eta = 1.0/np.max(EigValues) #learning rate
eta =0.1
Niterations = 30
momentum = 0.1

# Lists to store values for plotting
x_values = []
y_values_without_momentum = []
y_values_with_momentum = []

#core of the GD implementation
for iter in range(Niterations):
    gradient_without_momentum= (2.0/n)*X.T @ (X @ beta_without_momentum-y) #calculates the gradient of the cost function 
    beta_without_momentum -= eta*gradient_without_momentum #then updates beta in the opposite direction of the gradient
    #the learning rate eta scales the gradient to decide how much we update out parameters in each iteration. 

    gradient_with_momentum = (2.0 / n) * X.T @ (X @ beta_with_momentum - y)
    beta_with_momentum -= eta * gradient_with_momentum + momentum * gradient_with_momentum

     # Append values for plotting
    x_values.append(iter)
    y_values_without_momentum.append(np.sum((X @ beta_without_momentum - y)**2))
    y_values_with_momentum.append(np.sum((X @ beta_with_momentum - y)**2))
"""
print(beta)

xnew = np.array([[0],[2]])
xbnew = np.c_[np.ones((2,1)), xnew, xnew**2]
ypredict = xbnew.dot(beta)
ypredict2 = xbnew.dot(beta_linreg)

plt.plot(xnew, ypredict, "r-")
plt.plot(xnew, ypredict2, "b-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Gradient descent')
plt.show()
"""
# Plot the results for both cases
plt.plot(x_values, y_values_without_momentum, label='GD without Momentum', color='blue')
plt.plot(x_values, y_values_with_momentum, label='GD with Momentum', color='red')

plt.xlabel('Iterations')
plt.ylabel('Cost Function')
plt.title('Gradient Descent with and without Momentum')
plt.legend()
plt.show()
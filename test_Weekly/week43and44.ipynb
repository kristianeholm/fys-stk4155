{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "XOR Accuracy: 0.5\n",
      "OR Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "OR Accuracy: 0.75\n",
      "AND Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "AND Accuracy: 0.25\n",
      "XOR - Accuracy on training data: 50.00%\n",
      "OR - Accuracy on training data: 75.00%\n",
      "AND - Accuracy on training data: 25.00%\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#without scikit learn\n",
    "\"\"\"\n",
    "Simple code that tests XOR, OR and AND gates with linear regression\n",
    "\n",
    "Design matrix and the various output vectors for the different gates\n",
    "\"\"\"\n",
    "\n",
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from autograd import grad\n",
    "\n",
    "#sigmoid function = activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    # for backpropagation need activations in hidden and output layers\n",
    "    return a_h, probabilities\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    #error_output = probabilities - Y\n",
    "    # error in the hidden layer\n",
    "    #error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    error_output = probabilities - Y.reshape(-1, 1)  # Reshape Y to a column vector\n",
    "    error_hidden = np.dot(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "\n",
    "#cost function --> cross entropy for classification of binary cases\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * np.sum(target * np.log(X + 10e-10))\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Design matrix\n",
    "X = np.array([ [0, 0], [0, 1], [1, 0],[1, 1]],dtype=np.float64)\n",
    "\n",
    "#target values\n",
    "# XOR \n",
    "yXOR = np.array( [ 0, 1 ,1, 0])\n",
    "# AND \n",
    "yAND = np.array( [ 0, 0 ,0, 1])\n",
    "# OR \n",
    "yOR = np.array( [ 0, 1 ,1, 1])\n",
    "\n",
    "\n",
    "# Defining the neural network\n",
    "n_inputs, n_features = X.shape\n",
    "n_hidden_neurons = 2 #hidden nodes\n",
    "n_categories = 1 #the number of output categories --> number of output nodes in the neural network \n",
    "\n",
    "n_features = 2 #number of features/input dimensions in the dataset. \n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "output = feed_forward(X) \n",
    "\n",
    "#calculating the gradients needed for the backpropagation part\n",
    "cost_func = CostCrossEntropy #calculate cost using cost function\n",
    "cost_func_derivative = grad(cost_func(output)) #derivative of cost function with respect to the output\n",
    "#using autograd to automatically compute gradients --> want to apply these gradients for weight updates during backpropagation\n",
    "\n",
    "\n",
    "#XOR\n",
    "XORpredictions =np.round(output[1]) #xor predictions\n",
    "XORaccuracy = np.mean(XORpredictions == yXOR) #xor accuracy\n",
    "print(\"XOR Predictions:\", XORpredictions)\n",
    "print(\"XOR Accuracy:\", XORaccuracy)\n",
    "\n",
    "#OR\n",
    "ORpredtictions = np.round(output[1])\n",
    "ORaccuracy = np.mean(ORpredtictions == yOR)\n",
    "print(\"OR Predictions:\", ORpredtictions)\n",
    "print(\"OR Accuracy:\", ORaccuracy)\n",
    "\n",
    "#AND\n",
    "ANDpredictions = np.round(output[1])\n",
    "ANDaccuracy = np.mean(ANDpredictions == yAND)\n",
    "print(\"AND Predictions:\", ANDpredictions)\n",
    "print(\"AND Accuracy:\", ANDaccuracy)\n",
    "\n",
    "\n",
    "\n",
    "eta = 0.01 #learning rate\n",
    "lmbd = 0.01 #regularization parameter\n",
    "epochs_number = 100\n",
    "\n",
    "\n",
    "for gate_name, y_target in [(\"XOR\", yXOR), (\"OR\", yOR), (\"AND\", yAND)]:\n",
    "    # Reset weights and biases for each gate\n",
    "    hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "    hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "    output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "    output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "    for i in range(epochs_number):\n",
    "        # Calculate gradients based on the current gate's target variable\n",
    "        output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient = backpropagation(X, y_target)\n",
    "\n",
    "        # Apply regularization to the gradients\n",
    "        output_weights_gradient += lmbd * output_weights\n",
    "        hidden_weights_gradient += lmbd * hidden_weights\n",
    "\n",
    "        # Update weights and biases\n",
    "        output_weights -= eta * output_weights_gradient\n",
    "        output_bias -= eta * output_bias_gradient\n",
    "        hidden_weights -= eta * hidden_weights_gradient\n",
    "        hidden_bias -= eta * hidden_bias_gradient\n",
    "\n",
    "    # Calculate and print accuracy for the current gate\n",
    "    train_predictions = feed_forward(X)[1]\n",
    "    train_accuracy = np.mean(np.round(train_predictions) == y_target)\n",
    "    print(f\"{gate_name} - Accuracy on training data: {train_accuracy:.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the process of adjusting the weights of a neural network by analyzing the error rate from the previous iteration. It involves working backward from outputs to inputs to figure out how to reduce the number of errors and make a neural network more reliable. Type of supervised learning since it requires a known, desired output for each input value to caluculate the loss function gradient, which is how desired output values differ from actual output. \n",
    "\n",
    "An activatino function is a function that is added into an artificial NN in order to help the network lean complex patterns in the data. When comparing with a neuron-based model that is in out brains, the activatio function is at the end deciding what is to be fired to the next neuron. Takes the output signal from the previous cell and converts it into come form that can be taken as input to the next cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Design matrix\n",
    "X = np.array([ [0, 0], [0, 1], [1, 0],[1, 1]],dtype=np.float64)\n",
    "\n",
    "# The XOR gate\n",
    "yXOR = np.array( [ 0, 1 ,1, 0])\n",
    "# The OR gate\n",
    "yOR = np.array( [ 0, 1 ,1, 1])\n",
    "# The AND gate\n",
    "yAND = np.array( [ 0, 0 ,0, 1])\n",
    "\n",
    "# Defining the neural network\n",
    "n_hidden_neurons = 2\n",
    "\n",
    "eta_vals = np.logspace(-5, 1, 7)\n",
    "lmbd_vals = np.logspace(-5, 1, 7)\n",
    "# store models for later use\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "epochs = 100\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_hidden_neurons), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)\n",
    "        dnn.fit(X, yXOR)\n",
    "        DNN_scikit[i][j] = dnn\n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on data set: \", dnn.score(X, yXOR))\n",
    "        print()\n",
    "\n",
    "sns.set()\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "for i in range(len(eta_vals)):\n",
    "    for j in range(len(lmbd_vals)):\n",
    "        dnn = DNN_scikit[i][j]\n",
    "        test_pred = dnn.predict(X)\n",
    "        test_accuracy[i][j] = accuracy_score(yXOR, test_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
